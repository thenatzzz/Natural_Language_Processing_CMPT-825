{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to file (please specify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siyu Path\n",
    "PATH = \"/Users/wusiyu/Desktop/nlp-class-hw/zhsegment/\"\n",
    "\n",
    "# Nattapat Path\n",
    "PATH = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签 订 高 科 技 合 作 协 议\n",
      "新 华 社 上 海 八 月 三 十 一 日 电 （ 记 者 白 国 良 、 夏 儒 阁 ）\n",
      "“ 中 美 合 作 高 科 技 项 目 签 字 仪 式 ” 今 天 在 上 海 举 行 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(PATH+'data/count_1w.txt'))\n",
    "\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "\n",
    "with open(PATH+\"data/input/dev.txt\",encoding='utf8') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:3])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default score: 0.27\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open(PATH+'data/reference/dev.out', 'r',encoding='utf8') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"Default score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "#### Write some beautiful documentation of your program here. \n",
    "\n",
    "We are going to show our program below. \n",
    "For comparing with the default program, we will output the first three lines of the dev.txt as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议 \n",
      "\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白国良 、 夏儒阁 ） \n",
      "\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re, string, random, glob, operator, heapq, codecs, sys, optparse, os, logging, math\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "from math import log10\n",
    "\n",
    "# additional library\n",
    "import operator\n",
    "\n",
    "'''these indexes are for default segmentation function'''\n",
    "IDX_WORD = 0\n",
    "IDX_PROBABILITY = 1\n",
    "MAX_WORD_LENGTH =15\n",
    "\n",
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pw):\n",
    "        self.Pw = Pw\n",
    "\n",
    "    def Pwords(self, words):\n",
    "        \"The Probability of words.\"\n",
    "        return self.Pw(words)\n",
    "\n",
    "    def segment(self,text):\n",
    "        \"Return a list of words that is the best segmentation of text.\"\n",
    "        '''Dev score = 0.93 and took long time to run, implementaion using normal Dict'''\n",
    "\n",
    "        ''' dictionary as dynamic programming table'''\n",
    "        chart = {}\n",
    "\n",
    "        '''iterate through line of text'''\n",
    "        for idx_text in range(len(text)):\n",
    "\n",
    "            '''iterate and decide whether to add words to chart '''\n",
    "            for idx_word in range(1, MAX_WORD_LENGTH + 1):\n",
    "\n",
    "                '''continue if word length goes out of text length'''\n",
    "                if (idx_text - idx_word + 1) < 0:\n",
    "                    continue\n",
    "\n",
    "                '''get word from text'''\n",
    "                word = text[idx_text-idx_word+1:idx_text+1]\n",
    "\n",
    "                '''get probability of current word'''\n",
    "                prob = math.log(self.Pwords(word))\n",
    "\n",
    "                ''' check for previous word probability,\n",
    "                 if it exists we get probability of previous word, else we assign it to zero '''\n",
    "                if (idx_text - idx_word) >= 0:\n",
    "                    prev_prob = chart[idx_text - idx_word][IDX_PROBABILITY]\n",
    "                else:\n",
    "                    prev_prob = 0\n",
    "\n",
    "                '''dynamically update new prob'''\n",
    "                updated_prob = prob + prev_prob\n",
    "                '''check if text in chart or not OR updated probability is more than current probability,\n",
    "                  update chart with updated probability if the condition is True'''\n",
    "                if (idx_text not in chart) or (updated_prob > chart[idx_text][IDX_PROBABILITY]):\n",
    "                    chart[idx_text] = [word, prev_prob + prob]\n",
    "\n",
    "        ''' Get the best segmented text by iterate from the end index of our chart'''\n",
    "        endindex = len(text) - 1\n",
    "        segmented_text = []\n",
    "\n",
    "        while endindex >= 0:\n",
    "            word, prob = chart[endindex]\n",
    "            segmented_text.append(word)\n",
    "            endindex = endindex- len(word)\n",
    "\n",
    "        # return from end of array\n",
    "        return segmented_text[::-1]\n",
    "\n",
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "\n",
    "    def __call__(self, key):\n",
    "        if key in self:\n",
    "            return self[key]/self.N\n",
    "        else:\n",
    "            return self.missingfn(key, self.N)\n",
    "\n",
    "def datafile(name, sep='\\t'):\n",
    "    \"Read key,value pairs from file.\"\n",
    "    with open(name,encoding=\"utf8\") as fh:\n",
    "    # with open(name) as fh:\n",
    "        for line in fh:\n",
    "            (key, value) = line.split(sep)\n",
    "            yield (key, value)\n",
    "\n",
    "def punish_long_words(key, N,lambda_=0.03):\n",
    "    '''Function to assign probability to based on length of word\n",
    "    we can define lambda (hyperparameter) (default=0.03)'''\n",
    "    prob = (1.0/N) if len(key) <=1 else 1e-200+ pow(1.0/( lambda_*N), len(key))\n",
    "    return prob\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    Pw = Pdist(data=datafile(PATH+\"data/count_1w.txt\"),missingfn=punish_long_words)\n",
    "    \n",
    "    segmenter = Segment(Pw)\n",
    "    \n",
    "    i = 1\n",
    "    with open(PATH+\"data/input/dev.txt\",encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentence =\" \".join(segmenter.segment(line.strip()))\n",
    "            print(sentence,'\\n')\n",
    "            if i ==3:\n",
    "                break\n",
    "            i += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative segmentation: Dev score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Old Iterative segmentation dev score: 0.86\n",
      "Best Iterative segmentation dev score: 0.93\n"
     ]
    }
   ],
   "source": [
    "# ref data\n",
    "with open(PATH+'data/reference/dev.out', 'r',encoding='utf8') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "\n",
    "# iterative segemented output\n",
    "with open(PATH+\"dev_output/dev.out\",'r',encoding='utf8') as devh:\n",
    "    iterative_dev_data = [str(x).strip() for x in devh.read().splitlines()]\n",
    "with open(PATH+\"dev_output/dev_old.out\",'r',encoding='utf8') as devh:\n",
    "    old_iterative_dev_data = [str(x).strip() for x in devh.read().splitlines()]\n",
    "                \n",
    "tally = fscore(ref_data, iterative_dev_data)\n",
    "tally2 =fscore(ref_data, old_iterative_dev_data)\n",
    "print(\"Old Iterative segmentation dev score: {:.2f}\".format(tally2), file=sys.stderr)\n",
    "print(\"Best Iterative segmentation dev score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "#### Do some analysis of the results. What ideas did you try? What worked and what did not?\n",
    "\n",
    "There are two main steps we have done to increase the score of Chinese word segmentation. \n",
    "\n",
    "First of all, we implement new Segmentation function to replace the default function.The function decides to add new word to our table based on the calculated probability (this assignment we only use Unigram). Since we use Dynamic Programming approach instead of Greedy approach, we use just one main table to do all the work including storing our result. By using Segmentation function, the segmenter will not only identify each Chinese character as a single charactor, but also start to combine several Chinese character into one long word. The program will stop when we run through all character in the text input.\n",
    "\n",
    "Secondly, we have also done the simple Smoothing work. After all, not every word exists in our dictionary, which means that probability of some word could have been set to the default solution(1/N) and will not be added by our function. To avoid this situation, we include Smoothing function called 'punish_long_word' in our code. Basically, the main idea of this function is that the longer the lenght of unknown word, the lower the probability of that word will appear in our text. Additionally, the function allows us to set some hyperparameter (default=0.03). After Smoothing work, our code works even better as shown in new dev score (0.93)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def punish_long_words(key, N,lambda_=0.03):\n",
    "    '''Function to assign probability to based on length of word\n",
    "    we can define lambda (hyperparameter) (default=0.03)'''\n",
    "    prob = (1.0/N) if len(key) <=1 else 1e-200+ pow(1.0/( lambda_*N), len(key))\n",
    "    return prob\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
