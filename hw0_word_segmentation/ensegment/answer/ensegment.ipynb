{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensegment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ensegment import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "choose spain\n",
      "this is a test\n",
      "who represents\n",
      "experts exchange\n",
      "speed of art\n",
      "un climate change body\n",
      "we are the people\n",
      "mention your faves\n",
      "now playing\n",
      "the walking dead\n",
      "follow me\n",
      "we are the people\n",
      "mention your faves\n",
      "check domain\n",
      "bigrock\n",
      "name cheap\n",
      "apple domains\n",
      "honesty hour\n",
      "being human\n",
      "follow back\n",
      "social media\n",
      "30 seconds to earth\n",
      "current rate sought to go down\n",
      "this is insane\n",
      "what is my name\n",
      "is it time\n",
      "let us go\n",
      "me too\n",
      "now thatcher is dead\n",
      "advice for young journalists\n"
     ]
    }
   ],
   "source": [
    "# this number comes from Norvig's book (as we use the same data and codes based on him)\n",
    "N = 1024908267229\n",
    "\n",
    "# the file path should be defined according to count_1w.txt and dev.txt location\n",
    "Pw = Pdist(data=datafile(\"/Users/wusiyu/Desktop/nlp-class-hw/ensegment/data/count_1w.txt\"),N=N,missingfn=avoid_long_words)\n",
    "segmenter = Segment(Pw)\n",
    "with open(\"/Users/wusiyu/Desktop/nlp-class-hw/ensegment/data/input/dev.txt\") as f:\n",
    "    for line in f:\n",
    "        print(\" \".join(segmenter.segment(line.strip())))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "\n",
    "### Do some analysis of the results. What ideas did you try? What worked and what did not?\n",
    "\n",
    "Compared to the result of default code, there is an obvious improvement in dealing with long words.In the original result, some long words, such as \"unclimatechangebody\" and \"30secondstoearth\", are not be splited and some least common words are being ignored in the default solution.\n",
    "\n",
    "To solve the mentioned problem, we create a new function called \"avoid_long_words\". By applying this function, we assign some probability to the least common words instead of assigning them with an equal probability as shown in the default solution. The function works as the following: as the length of the word becomes longer, the probability will be lower (as we put length of word in denominator when calculating probability.)  \n",
    "\n",
    "This function successfully splits many long words, such as \"mentionyourfaves\" and \"nowthatcherisdead\". However, it still fails to split one word, which is \"bigrock\". One explaination is that the length of \"bigrock\" is not too long, so the probability is not penalized to be very low. Also, the solution could be improved if we use conditional bigram language model rather than simple unigram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
