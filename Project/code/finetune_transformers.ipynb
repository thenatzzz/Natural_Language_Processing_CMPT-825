{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"TPU","colab":{"name":"finetune_transformers.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.7"},"varInspector":{"cols":{"lenName":16,"lenType":16,"lenVar":40},"kernels_config":{"python":{"delete_cmd_postfix":"","delete_cmd_prefix":"del ","library":"var_list.py","varRefreshCmd":"print(var_dic_list())"},"r":{"delete_cmd_postfix":") ","delete_cmd_prefix":"rm(","library":"var_list.r","varRefreshCmd":"cat(var_dic_list()) "}},"types_to_exclude":["module","function","builtin_function_or_method","instance","_Feature"],"window_display":false}},"cells":[{"cell_type":"markdown","metadata":{"id":"1EDP7JKqzJyf"},"source":["# Fine Tuning Transformer for News Summarization\n"]},{"cell_type":"code","metadata":{"id":"WD_vnyLXZQzD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1605731255214,"user_tz":480,"elapsed":56282,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"e4a7647c-29b1-4997-8382-3f026b1a8097"},"source":["# credit: https://github.com/abhimishra91/transformers-tutorials/blob/master/transformers_summarization_wandb.ipynb\n","!pip install transformers -q\n","!pip install wandb -q\n","\n","# Code for TPU packages install\n","# !curl -q https://raw.githubusercontent.com/pytorch/xla/master/contrib/scripts/env-setup.py -o pytorch-xla-env-setup.py\n","# !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev"],"execution_count":1,"outputs":[{"output_type":"stream","text":["  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n","                                 Dload  Upload   Total   Spent    Left  Speed\n","100  5116  100  5116    0     0  25326      0 --:--:-- --:--:-- --:--:-- 25708\n","Updating... This may take around 2 minutes.\n","Updating TPU runtime to pytorch-dev20200515 ...\n","Uninstalling torch-1.6.0a0+bf2bbd9:\n","  Successfully uninstalled torch-1.6.0a0+bf2bbd9\n","Uninstalling torchvision-0.7.0a0+a6073f0:\n","  Successfully uninstalled torchvision-0.7.0a0+a6073f0\n","Copying gs://tpu-pytorch/wheels/torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n","- [1 files][ 91.0 MiB/ 91.0 MiB]                                                \n","Operation completed over 1 objects/91.0 MiB.                                     \n","Copying gs://tpu-pytorch/wheels/torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n","\\ [1 files][119.5 MiB/119.5 MiB]                                                \n","Operation completed over 1 objects/119.5 MiB.                                    \n","Copying gs://tpu-pytorch/wheels/torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl...\n","\n","Operation completed over 1 objects/2.3 MiB.                                      \n","Processing ./torch-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (0.16.0)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch==nightly+20200515) (1.18.5)\n","Done updating TPU runtime\n","\u001b[31mERROR: fastai 1.0.61 requires torchvision, which is not installed.\u001b[0m\n","Installing collected packages: torch\n","Successfully installed torch-1.6.0a0+bf2bbd9\n","Processing ./torch_xla-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n","Installing collected packages: torch-xla\n","  Found existing installation: torch-xla 1.6+2b2085a\n","    Uninstalling torch-xla-1.6+2b2085a:\n","      Successfully uninstalled torch-xla-1.6+2b2085a\n","Successfully installed torch-xla-1.6+2b2085a\n","Processing ./torchvision-nightly+20200515-cp36-cp36m-linux_x86_64.whl\n","Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (7.0.0)\n","Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.6.0a0+bf2bbd9)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==nightly+20200515) (1.18.5)\n","Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch->torchvision==nightly+20200515) (0.16.0)\n","Installing collected packages: torchvision\n","Successfully installed torchvision-0.7.0a0+a6073f0\n","Reading package lists... Done\n","Building dependency tree       \n","Reading state information... Done\n","libomp5 is already the newest version (5.0.1-1).\n","libopenblas-dev is already the newest version (0.2.20+ds-4).\n","0 upgraded, 0 newly installed, 0 to remove and 14 not upgraded.\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"pzM1_ykHaFur","executionInfo":{"status":"ok","timestamp":1605731258406,"user_tz":480,"elapsed":59445,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["# Importing stock libraries\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n","\n","# Importing the T5 modules from huggingface/transformers\n","from transformers import T5Tokenizer, T5ForConditionalGeneration\n","\n","# WandB – Import the wandb library\n","import wandb"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"KvPxXdKJguYB","executionInfo":{"status":"ok","timestamp":1605731258408,"user_tz":480,"elapsed":59428,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"40cf67f9-f3ce-4b28-c80c-d93a8e4d4d1d"},"source":["# Checking out the GPU we have access to. This is output is from the google colab version. \n","!nvidia-smi"],"execution_count":3,"outputs":[{"output_type":"stream","text":["NVIDIA-SMI has failed because it couldn't communicate with the NVIDIA driver. Make sure that the latest NVIDIA driver is installed and running.\n","\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"NLxxwd1scQNv","executionInfo":{"status":"ok","timestamp":1605731270919,"user_tz":480,"elapsed":71919,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["# # Setting up the device for GPU usage\n","from torch import cuda\n","device = 'cuda' if cuda.is_available() else 'cpu'\n","\n","# Preparing for TPU usage\n","# import torch_xla\n","# import torch_xla.core.xla_model as xm\n","# device = xm.xla_device()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"L-ePh9dEKXMw","executionInfo":{"status":"ok","timestamp":1605731272279,"user_tz":480,"elapsed":73259,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"cb770295-3707-43a1-afb3-7d427d7aac84"},"source":["# Login to wandb to log the model run and all the parameters\n","!wandb login"],"execution_count":5,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthenatzzz\u001b[0m (use `wandb login --relogin` to force relogin)\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"932p8NhxeNw4","executionInfo":{"status":"ok","timestamp":1605731272282,"user_tz":480,"elapsed":73238,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["# Creating a custom dataset for reading the dataframe and loading it into the dataloader to pass it to the neural network at a later stage for finetuning the model and to prepare it for predictions\n","\n","class CustomDataset(Dataset):\n","\n","    def __init__(self, dataframe, tokenizer, source_len, summ_len):\n","        self.tokenizer = tokenizer\n","        self.data = dataframe\n","        self.source_len = source_len\n","        self.summ_len = summ_len\n","        self.text = self.data.text\n","        self.ctext = self.data.ctext\n","\n","    def __len__(self):\n","        return len(self.text)\n","\n","    def __getitem__(self, index):\n","        ctext = str(self.ctext[index])\n","        ctext = ' '.join(ctext.split())\n","\n","        text = str(self.text[index])\n","        text = ' '.join(text.split())\n","\n","        source = self.tokenizer.batch_encode_plus([ctext], max_length= self.source_len, pad_to_max_length=True,return_tensors='pt')\n","        target = self.tokenizer.batch_encode_plus([text], max_length= self.summ_len, pad_to_max_length=True,return_tensors='pt')\n","\n","        source_ids = source['input_ids'].squeeze()\n","        source_mask = source['attention_mask'].squeeze()\n","        target_ids = target['input_ids'].squeeze()\n","        target_mask = target['attention_mask'].squeeze()\n","\n","        return {\n","            'source_ids': source_ids.to(dtype=torch.long), \n","            'source_mask': source_mask.to(dtype=torch.long), \n","            'target_ids': target_ids.to(dtype=torch.long),\n","            'target_ids_y': target_ids.to(dtype=torch.long)\n","        }"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaPAR7TWmxoM","executionInfo":{"status":"ok","timestamp":1605731272284,"user_tz":480,"elapsed":73222,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["# Creating the training function. This will be called in the main function. It is run depending on the epoch value.\n","# The model is put into train mode and then we wnumerate over the training loader and passed to the defined network \n","\n","def train(epoch, tokenizer, model, device, loader, optimizer):\n","    model.train()\n","    for _,data in enumerate(loader, 0):\n","        y = data['target_ids'].to(device, dtype = torch.long)\n","        y_ids = y[:, :-1].contiguous()\n","        lm_labels = y[:, 1:].clone().detach()\n","        lm_labels[y[:, 1:] == tokenizer.pad_token_id] = -100\n","        ids = data['source_ids'].to(device, dtype = torch.long)\n","        mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","        outputs = model(input_ids = ids, attention_mask = mask, decoder_input_ids=y_ids, lm_labels=lm_labels)\n","        loss = outputs[0]\n","        \n","        if _%10 == 0:\n","            wandb.log({\"Training Loss\": loss.item()})\n","\n","        if _%500==0:\n","            print(f'Epoch: {epoch}, Loss:  {loss.item()}')\n","        \n","        optimizer.zero_grad()\n","        loss.backward()\n","        optimizer.step()\n","        # xm.optimizer_step(optimizer)\n","        # xm.mark_step()"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"j9TNdHlQ0CLz","executionInfo":{"status":"ok","timestamp":1605731272285,"user_tz":480,"elapsed":73202,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}}},"source":["def validate(epoch, tokenizer, model, device, loader):\n","    model.eval()\n","    predictions = []\n","    actuals = []\n","    with torch.no_grad():\n","        for _, data in enumerate(loader, 0):\n","            y = data['target_ids'].to(device, dtype = torch.long)\n","            ids = data['source_ids'].to(device, dtype = torch.long)\n","            mask = data['source_mask'].to(device, dtype = torch.long)\n","\n","            generated_ids = model.generate(\n","                input_ids = ids,\n","                attention_mask = mask, \n","                max_length=150, \n","                num_beams=2,\n","                repetition_penalty=2.5, \n","                length_penalty=1.0, \n","                early_stopping=True\n","                )\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=True) for g in generated_ids]\n","            target = [tokenizer.decode(t, skip_special_tokens=True, clean_up_tokenization_spaces=True)for t in y]\n","            if _%100==0:\n","                print(f'Completed {_}')\n","\n","            predictions.extend(preds)\n","            actuals.extend(target)\n","    return predictions, actuals"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Hn8VMbXI-yyM","executionInfo":{"status":"ok","timestamp":1605731307480,"user_tz":480,"elapsed":108353,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"653d12ed-7482-4547-c2e7-c1f20a4b604d"},"source":["import pickle\n","import copy\n","path_cnn= '/content/drive/My Drive/Colab Notebooks/data/train_dataset.pkl'\n","\n","stories = pickle.load(open(path_cnn, 'rb'))\n","print(\"total number of CNN data: \",len(stories))\n","\n","# number of highlights to be used\n","NUM_HIGHLIGHT= 1\n","# number of storie sentences to be used\n","NUM_STORY = 2\n","\n","# join sentence in both stories and highlights together for each data sample\n","processed_stories = copy.deepcopy(stories)\n","\n","# join stories and highlights into 2 column pd dataframe\n","for each_story in processed_stories:\n","  # join highlights\n","  each_story['highlights'] = ' '.join(each_story['highlights'][0:NUM_HIGHLIGHT])\n","  # join story sentences\n","  each_story['story'] = ' '.join(each_story['story'][0:NUM_STORY])\n"],"execution_count":10,"outputs":[{"output_type":"stream","text":["total number of CNN data:  301956\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":572},"id":"bum-mMwg-8fi","executionInfo":{"status":"ok","timestamp":1605731308206,"user_tz":480,"elapsed":109058,"user":{"displayName":"Nattapat Juthaprachakul","photoUrl":"","userId":"05256019876215417123"}},"outputId":"927d1b8e-920e-4f54-ab5c-d77f7f18b0c1"},"source":["df_cnn = pd.DataFrame(processed_stories)\n","df_cnn['highlights'] = 'summarize: ' + df_cnn['highlights']\n","\n","df_cnn\n"],"execution_count":11,"outputs":[{"output_type":"execute_result","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>story</th>\n","      <th>highlights</th>\n","      <th>id</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>controversial malawi president bingu wa muthar...</td>\n","      <td>summarize: controversial leader collapsed on t...</td>\n","      <td>id_dm_143548</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>arizonans thats who it figures in the immigrat...</td>\n","      <td>summarize: arizona judge rules for group tryin...</td>\n","      <td>id_cnn_66720</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>northwestern universitys football players vote...</td>\n","      <td>summarize: northwestern university football pl...</td>\n","      <td>id_cnn_91222</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>a european union delegation met saturday with ...</td>\n","      <td>summarize: of eu meeting mugabe said there was...</td>\n","      <td>id_cnn_53005</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>world player of the year lionel messi grabbed ...</td>\n","      <td>summarize: lionel messi continues his incredib...</td>\n","      <td>id_cnn_21168</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>301951</th>\n","      <td>newcastle legend alan shearer has slammed the ...</td>\n","      <td>summarize: newcastle are currently in the prem...</td>\n","      <td>id_dm_175879</td>\n","    </tr>\n","    <tr>\n","      <th>301952</th>\n","      <td>the mayor of two romanian villages has claimed...</td>\n","      <td>summarize: remus neda is heading for the uk to...</td>\n","      <td>id_dm_29988</td>\n","    </tr>\n","    <tr>\n","      <th>301953</th>\n","      <td>the health of pope emeritus benedict xvi has d...</td>\n","      <td>summarize: respected vatican expert sparks fea...</td>\n","      <td>id_dm_162103</td>\n","    </tr>\n","    <tr>\n","      <th>301954</th>\n","      <td>like the moment that harry potter first learne...</td>\n","      <td>summarize: many ireporters feel a kinship to h...</td>\n","      <td>id_cnn_27915</td>\n","    </tr>\n","    <tr>\n","      <th>301955</th>\n","      <td>much of the country has turned up the heat dur...</td>\n","      <td>summarize: fastmoving cold front knifes throug...</td>\n","      <td>id_cnn_2259</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>301956 rows × 3 columns</p>\n","</div>"],"text/plain":["                                                    story  ...            id\n","0       controversial malawi president bingu wa muthar...  ...  id_dm_143548\n","1       arizonans thats who it figures in the immigrat...  ...  id_cnn_66720\n","2       northwestern universitys football players vote...  ...  id_cnn_91222\n","3       a european union delegation met saturday with ...  ...  id_cnn_53005\n","4       world player of the year lionel messi grabbed ...  ...  id_cnn_21168\n","...                                                   ...  ...           ...\n","301951  newcastle legend alan shearer has slammed the ...  ...  id_dm_175879\n","301952  the mayor of two romanian villages has claimed...  ...   id_dm_29988\n","301953  the health of pope emeritus benedict xvi has d...  ...  id_dm_162103\n","301954  like the moment that harry potter first learne...  ...  id_cnn_27915\n","301955  much of the country has turned up the heat dur...  ...   id_cnn_2259\n","\n","[301956 rows x 3 columns]"]},"metadata":{"tags":[]},"execution_count":11}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":321},"id":"ZtNs9ytpCow2","outputId":"cef6e23a-815c-4408-d263-94c8ef931dfb"},"source":["def main():\n","    # WandB – Initialize a new run\n","    wandb.init(project=\"transformers_tutorials_summarization\")\n","\n","    # WandB – Config is a variable that holds and saves hyperparameters and inputs\n","    # Defining some key variables that will be used later on in the training  \n","    config = wandb.config          # Initialize config\n","    # config.TRAIN_BATCH_SIZE = 2    # input batch size for training (default: 64)\n","    config.TRAIN_BATCH_SIZE = 1024    # input batch size for training (default: 64)\n","    # config.VALID_BATCH_SIZE = 2    # input batch size for testing (default: 1000)\n","    config.VALID_BATCH_SIZE = 1000    # input batch size for testing (default: 1000)\n","    config.TRAIN_EPOCHS = 2        # number of epochs to train (default: 10)\n","    config.VAL_EPOCHS = 1 \n","    config.LEARNING_RATE = 1e-4    # learning rate (default: 0.01)\n","    config.SEED = 42               # random seed (default: 42)\n","    # config.MAX_LEN = 512\n","    # config.SUMMARY_LEN = 150 \n","    config.MAX_LEN = 100\n","    config.SUMMARY_LEN = 20 \n","\n","    # Set random seeds and deterministic pytorch for reproducibility\n","    torch.manual_seed(config.SEED) # pytorch random seed\n","    np.random.seed(config.SEED) # numpy random seed\n","    torch.backends.cudnn.deterministic = True\n","\n","    # tokenzier for encoding the text\n","    tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n","    \n","\n","    # Importing and Pre-Processing the domain data\n","    # Selecting the needed columns only. \n","    # Adding the summarzie text in front of the text. This is to format the dataset similar to how T5 model was trained for summarization task. \n","    # df = pd.read_csv('/content/drive/MyDrive/Colab Notebooks/data/unrelated/news_summary.csv',encoding='latin-1')\n","    # df = df[['text','ctext']]\n","    # df.ctext = 'summarize: ' + df.ctext\n","    # print(df.head())\n","    df=df_cnn.copy()\n","    df= df[['story','highlights']]\n","    df['text'] = df['story']\n","    df['ctext'] = df['highlights']\n","    \n","    # Creation of Dataset and Dataloader\n","    # Defining the train size. So 80% of the data will be used for training and the rest will be used for validation. \n","    train_size = 0.8\n","    train_dataset=df.sample(frac=train_size,random_state = config.SEED)\n","    val_dataset=df.drop(train_dataset.index).reset_index(drop=True)\n","    train_dataset = train_dataset.reset_index(drop=True)\n","\n","    print(\"FULL Dataset: {}\".format(df.shape))\n","    print(\"TRAIN Dataset: {}\".format(train_dataset.shape))\n","    print(\"TEST Dataset: {}\".format(val_dataset.shape))\n","\n","\n","    # Creating the Training and Validation dataset for further creation of Dataloader\n","    training_set = CustomDataset(train_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","    val_set = CustomDataset(val_dataset, tokenizer, config.MAX_LEN, config.SUMMARY_LEN)\n","\n","    # Defining the parameters for creation of dataloaders\n","    train_params = {\n","        'batch_size': config.TRAIN_BATCH_SIZE,\n","        'shuffle': True,\n","        'num_workers': 0\n","        }\n","\n","    val_params = {\n","        'batch_size': config.VALID_BATCH_SIZE,\n","        'shuffle': False,\n","        'num_workers': 0\n","        }\n","\n","    # Creation of Dataloaders for testing and validation. This will be used down for training and validation stage for the model.\n","    training_loader = DataLoader(training_set, **train_params)\n","    val_loader = DataLoader(val_set, **val_params)\n","\n","\n","    \n","    # Defining the model. We are using t5-base model and added a Language model layer on top for generation of Summary. \n","    # Further this model is sent to device (GPU/TPU) for using the hardware.\n","    model = T5ForConditionalGeneration.from_pretrained(\"t5-base\")\n","    model = model.to(device)\n","\n","    # Defining the optimizer that will be used to tune the weights of the network in the training session. \n","    optimizer = torch.optim.Adam(params =  model.parameters(), lr=config.LEARNING_RATE)\n","\n","    # Log metrics with wandb\n","    wandb.watch(model, log=\"all\")\n","    # Training loop\n","    print('Initiating Fine-Tuning for the model on our dataset')\n","\n","    for epoch in range(config.TRAIN_EPOCHS):\n","        train(epoch, tokenizer, model, device, training_loader, optimizer)\n","\n","\n","    # Validation loop and saving the resulting file with predictions and acutals in a dataframe.\n","    # Saving the dataframe as predictions.csv\n","    print('Now generating summaries on our fine tuned model for the validation dataset and saving it in a dataframe')\n","    for epoch in range(config.VAL_EPOCHS):\n","        predictions, actuals = validate(epoch, tokenizer, model, device, val_loader)\n","        final_df = pd.DataFrame({'Generated Text':predictions,'Actual Text':actuals})\n","        final_df.to_csv('./models/predictions.csv')\n","        print('Output Files generated for review')\n","\n","if __name__ == '__main__':\n","    main()"],"execution_count":null,"outputs":[{"output_type":"stream","text":["\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mthenatzzz\u001b[0m (use `wandb login --relogin` to force relogin)\n"],"name":"stderr"},{"output_type":"display_data","data":{"text/html":["\n","                Tracking run with wandb version 0.10.10<br/>\n","                Syncing run <strong style=\"color:#cdcd00\">major-sun-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n","                Project page: <a href=\"https://wandb.ai/thenatzzz/transformers_tutorials_summarization\" target=\"_blank\">https://wandb.ai/thenatzzz/transformers_tutorials_summarization</a><br/>\n","                Run page: <a href=\"https://wandb.ai/thenatzzz/transformers_tutorials_summarization/runs/16gri4jx\" target=\"_blank\">https://wandb.ai/thenatzzz/transformers_tutorials_summarization/runs/16gri4jx</a><br/>\n","                Run data is saved locally in <code>/content/wandb/run-20201118_202828-16gri4jx</code><br/><br/>\n","            "],"text/plain":["<IPython.core.display.HTML object>"]},"metadata":{"tags":[]}},{"output_type":"stream","text":["FULL Dataset: (301956, 4)\n","TRAIN Dataset: (241565, 4)\n","TEST Dataset: (60391, 4)\n"],"name":"stdout"},{"output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"],"name":"stderr"},{"output_type":"stream","text":["Initiating Fine-Tuning for the model on our dataset\n"],"name":"stdout"},{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/transformers/tokenization_utils_base.py:2022: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  FutureWarning,\n","/usr/local/lib/python3.6/dist-packages/transformers/modeling_t5.py:1156: FutureWarning: The `lm_labels` argument is deprecated and will be removed in a future version, use `labels` instead.\n","  FutureWarning,\n"],"name":"stderr"},{"output_type":"stream","text":["Epoch: 0, Loss:  8.309088706970215\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"zXBOfP0-0ZEi"},"source":["from google.colab import drive\n","drive.mount('/content/drive')"],"execution_count":null,"outputs":[]}]}