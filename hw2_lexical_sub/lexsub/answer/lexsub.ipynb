{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=27.89\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "#### Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LexSub:\n",
    "    def __init__(self, wvec_file, topn=10):\n",
    "        self.wvecs = pymagnitude.Magnitude(wvec_file)\n",
    "        self.topn = topn\n",
    "\n",
    "    def substitutes(self, index, sentence):\n",
    "        \"Return ten guesses that are appropriate lexical substitutions for the word at sentence[index].\"\n",
    "        word = sentence[index]\n",
    "\n",
    "        return (list(map(lambda k: k[0], self.wvecs.most_similar(word, topn=self.topn))))\n",
    "\n",
    "def retrofit(wvecs, lexicon, iteration=10):\n",
    "    '''copy val from old vec to new word vector'''\n",
    "    new_wvecs = deepcopy(wvecs)\n",
    "\n",
    "    '''get unique vocab names'''\n",
    "    wvec_dict = set(new_wvecs.keys())\n",
    "\n",
    "    '''get list of mutual/intersected word between Lexicon and the embedding keys/words'''\n",
    "    loop_vocab = wvec_dict.intersection(set(lexicon.keys()))\n",
    "\n",
    "    ''' iterate based on number of time we want to update'''\n",
    "    for iter in range(iteration):\n",
    "        '''loop through every node also in ontology (else just use estimation)'''\n",
    "        for word in loop_vocab:\n",
    "            '''get list of neighbor words (from Lexicon) that match the top N most similar word'''\n",
    "            word_neighbours = set(lexicon[word]).intersection(wvec_dict)\n",
    "            num_neighbours= len(word_neighbours)\n",
    "\n",
    "            if num_neighbours == 0:\n",
    "                continue\n",
    "\n",
    "            '''create new vec and estimate new vector according to neighbors'''\n",
    "            new_vec = num_neighbours * wvecs[word]\n",
    "            '''iterate every neighbor word and calculate according to equation'''\n",
    "            # hyperparameter\n",
    "            ALPHA = 0.8\n",
    "            for pp_word in word_neighbours:\n",
    "                dis = calculate_cosine_sim(new_wvecs[pp_word], wvecs[word])\n",
    "                new_vec += ((dis+ALPHA)*new_wvecs[pp_word])\n",
    "            new_wvecs[word] = new_vec/(2*num_neighbours)\n",
    "    return new_wvecs\n",
    "\n",
    "'''Helper function'''\n",
    "def calculate_cosine_sim(vect1,vect2):\n",
    "    return dot(vect1, vect2)/(norm(vect1)*norm(vect2))\n",
    "\n",
    "\n",
    "def load_Glove_to_dict(file_glove):\n",
    "    '''Load Glove to dictionary (key=word,value=vector)'''\n",
    "    print(\"Start loading Glove Model from Stanford Glove.txt\")\n",
    "\n",
    "    file = open(file_glove,'r',encoding='utf-8')\n",
    "    glove_dict = {}\n",
    "\n",
    "    for line in tqdm.tqdm(file):\n",
    "        split_lines = line.split()\n",
    "        word = split_lines[0]\n",
    "        word_embedding = np.array([float(value) for value in split_lines[1:]])\n",
    "        glove_dict[word] = word_embedding\n",
    "\n",
    "    print(len(glove_dict),\" words of Glove loaded successful!\")\n",
    "    return glove_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    optparser = optparse.OptionParser()\n",
    "    optparser.add_option(\"-i\", \"--inputfile\", dest=\"input\", default=os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data', 'input',\n",
    "                                              'dev.txt'), help=\"input file with target word in context\")\n",
    "    optparser.add_option(\"-w\", \"--wordvecfile\", dest=\"wordvecfile\", default=os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub','answer', 'data', 'glove.6B.100d.txt'), help=\"word vectors file\")\n",
    "    optparser.add_option(\"-n\", \"--topn\", dest=\"topn\", default=10, help=\"produce these many guesses\")\n",
    "    optparser.add_option(\"-l\", \"--logfile\", dest=\"logfile\", default=None, help=\"log file for debugging\")\n",
    "    optparser.add_option(\"-L\", \"--lexiconfile\", dest=\"lexicon\", default=os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data',\n",
    "                                              'lexicons', 'wordnet-synonyms.txt'), help=\"lexicon file\")\n",
    "    optparser.add_option(\"-r\", \"--retrofitted_vecfile\", dest=\"retrofitted_vecfile\", default=os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data',\n",
    "                                              'retrofitted.glove.magnitude'), help=\"load retrofited embedding\")\n",
    "\n",
    "    (opts, _) = optparser.parse_args()\n",
    "\n",
    "    if opts.logfile is not None:\n",
    "        logging.basicConfig(filename=opts.logfile, filemode='w', level=logging.DEBUG)\n",
    "\n",
    "    '''get lexicon and format it'''\n",
    "    lexicon = read_lexicon(opts.lexicon)\n",
    "\n",
    "    ''' if we don't have retrofitted file, we retrofit the Glove vectors else we just load.\n",
    "        We retrofitted Glove then save its embedding for later use (txt file) '''\n",
    "    if os.path.isfile(opts.retrofitted_vecfile) == False:\n",
    "\n",
    "        print(\"\\nRetrofitted embedding file does not exist. Let's retrofit !\\n\")\n",
    "\n",
    "        glove_dict = load_Glove_to_dict(opts.wordvecfile)\n",
    "        retrofitted_vec = retrofit(glove_dict,lexicon, iteration=10)\n",
    "\n",
    "        file_loc = 'data/retrofitted_glove.txt'\n",
    "        save_embedding(retrofitted_vec,file_loc)\n",
    "\n",
    "        print('\\nSuccessfully retrofitting embedding! and save to {}.'.format(file_loc))\n",
    "        print('-'*50)\n",
    "        print(\"PLEASE run pymagnitude.converter to convert .txt file to .magnitude file\")\n",
    "        print('-'*50)\n",
    "\n",
    "    else:\n",
    "        lexsub = LexSub(opts.retrofitted_vecfile, int(opts.topn))\n",
    "\n",
    "        num_lines = sum(1 for line in open(opts.input,'r',encoding='utf-8-sig'))\n",
    "\n",
    "        with open(opts.input,encoding='utf-8-sig') as f:\n",
    "            # for line in tqdm.tqdm(f, total=num_lines):\n",
    "            for line in f:\n",
    "                fields = line.strip().split('\\t')\n",
    "                print(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Procedures to run this code\n",
    "#### 1. We need Glove.txt file where we can download from Stanford website or we can just convert Glove.magnitude to .txt file.\n",
    "#### 2. At first, the program will check whether we have retrofitted Glove embedding or not. \n",
    "   \n",
    "####   If not, the program will:\n",
    "  \n",
    "   2.1) load up Glove.txt, \n",
    "   \n",
    "   2.2) retrofit those embedding vectors( we can identify the number of iterations, other embedding vectors besides Glove, and lexical files besides the default files), and \n",
    "   \n",
    "   2.3) save into new file (retrofitted_glove.txt).\n",
    "    AND the program will EXIT.\n",
    "\n",
    "#### 3. If we don't have Retrofitted embedding in the format of Pymagnitude yet (since we have retrofitted embedding in form .txt from procedure 2):\n",
    "\n",
    "     We can convert it using : \n",
    "     \n",
    "     python3 -m pymagnitude.converter -i data/retrofitted_glove.txt -o data/retrofitted.glove.magnitude\n",
    "     \n",
    "#### 4. Rerun this code again, and the program will use our retrofitted embedding to operate on our input text file. (dev/test.txt)\n",
    "\n",
    "The program will load embedding vectors into dictionary where the keys store unique words and values store vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our solution on the dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "english edge position line place point way front while face\n",
      "english edge position line place point way front while face\n",
      "english edge position line place point way front while face\n",
      "english edge position line place point way front while face\n",
      "english edge position line place point way front while face\n",
      "english edge position line place point way front while face\n",
      "english edge position line place point way front while face\n",
      "english edge position line place point way front while face\n",
      "english edge position line place point way front while face\n",
      "english edge position line place point way front while face\n"
     ]
    }
   ],
   "source": [
    "from lexsub import *\n",
    "import os\n",
    "\n",
    "# lexsub = LexSub(os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data',\n",
    "#                                               'retrofitted.glove.magnitude'))\n",
    "# output = []\n",
    "# with open(os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data', 'input',\n",
    "#                                               'dev.txt')) as f:\n",
    "# lexsub = LexSub(os.path.join('/d','Coding','SFU_CA','CMPT-825','nlpclass-1207-g-oasis','hw2','lexsub','data','retrofitted_glove.magnitude'))\n",
    "\n",
    "lexsub = LexSub(os.path.join('data/retrofitted_glove.magnitude'))\n",
    "\n",
    "output = []\n",
    "with open('data/input/dev.txt') as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True positive:  842.0  False positive: 861.0\n",
      "Score=49.44\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "# with open(os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data', 'reference','dev.out'), 'rt') as refh:\n",
    "with open('data/reference/dev.out', 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "#### Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    \n",
    "   The most important thing we did in this task is to implement Retrofitting function and add some modification to it as to improve overall performance. After applying the Retrofitting function with wordnet_synonyms.txt and 10 iterations, the accuracy score on the dev.txt file from our new code could reach around 46.10. \n",
    "    \n",
    "   To improve the performance, we modified the original function by adding some values calculated from Cosine similarity between the words in vocaburary and lexical synonym words around our target word to improve the performance of original Retrofitting function. \n",
    "\n",
    "   We select the Cosine distance between two vectors as the criteria in our model. Also, the calculated values can be adjusted according to an arbitrary weight(ALPHA) (hyperparameter). We found out that ALPHA =0.8 can achieve the accuracy of 49.44 with the same iteration and lexicon file.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrofit(wvecs, lexicon, iteration=10):\n",
    "    '''copy val from old vec to new word vector'''\n",
    "    new_wvecs = deepcopy(wvecs)\n",
    "\n",
    "    '''get unique vocab names'''\n",
    "    wvec_dict = set(new_wvecs.keys())\n",
    "\n",
    "    '''get list of mutual/intersected word between Lexicon and the embedding keys/words'''\n",
    "    loop_vocab = wvec_dict.intersection(set(lexicon.keys()))\n",
    "\n",
    "    ''' iterate based on number of time we want to update'''\n",
    "    for iter in range(iteration):\n",
    "        '''loop through every node also in ontology (else just use estimation)'''\n",
    "        for word in loop_vocab:\n",
    "            '''get list of neighbor words (from Lexicon) that match the top N most similar word'''\n",
    "            word_neighbours = set(lexicon[word]).intersection(wvec_dict)\n",
    "            num_neighbours= len(word_neighbours)\n",
    "\n",
    "            if num_neighbours == 0:\n",
    "                continue\n",
    "\n",
    "            '''create new vec and estimate new vector according to neighbors'''\n",
    "            new_vec = num_neighbours * wvecs[word]\n",
    "            '''iterate every neighbor word and calculate according to equation'''\n",
    "            # hyperparameter\n",
    "            ALPHA = 0.8\n",
    "            for pp_word in word_neighbours:\n",
    "                dis = calculate_cosine_sim(new_wvecs[pp_word], wvecs[word])\n",
    "                new_vec += ((dis+ALPHA)*new_wvecs[pp_word])\n",
    "            new_wvecs[word] = new_vec/(2*num_neighbours)\n",
    "    return new_wvecs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nonetheless, we have tried to consider Context words by applying Window sizes of 5 around our main word and use some models to generate new vectors such as AddBal. However, it turned out to have a negative impact on our result and add substantial time for the model training; therefore, we sticked to our simple approach. For the futuew work, different approaches of selecting Context words and setting hyperparameters like Window size could impact results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
