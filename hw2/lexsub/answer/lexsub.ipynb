{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lexsub: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n",
      "sides edge bottom front club line both back place corner\n"
     ]
    }
   ],
   "source": [
    "lexsub = LexSub(os.path.join('data','glove.6B.100d.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('data','input','dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=27.89\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('data','reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, optparse\n",
    "import tqdm\n",
    "import pymagnitude\n",
    "\n",
    "import re\n",
    "from copy import deepcopy\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class LexSub:\n",
    "    def __init__(self, wvec_file, topn=10):\n",
    "        self.wvecs = pymagnitude.Magnitude(wvec_file)\n",
    "        self.topn = topn\n",
    "\n",
    "    def substitutes(self, index, sentence):\n",
    "        \"Return ten guesses that are appropriate lexical substitutions for the word at sentence[index].\"\n",
    "        word = sentence[index]\n",
    "\n",
    "        return (list(map(lambda k: k[0], self.wvecs.most_similar(word, topn=self.topn))))\n",
    "\n",
    "def retrofit(wvecs, lexicon, iteration=10):\n",
    "    '''copy val from old vec to new word vector'''\n",
    "    new_wvecs = deepcopy(wvecs)\n",
    "\n",
    "    '''get unique vocab names'''\n",
    "    wvec_dict = set(new_wvecs.keys())\n",
    "\n",
    "    '''get list of mutual/intersected word between Lexicon and the embedding keys/words'''\n",
    "    loop_vocab = wvec_dict.intersection(set(lexicon.keys()))\n",
    "\n",
    "    ''' iterate based on number of time we want to update'''\n",
    "    for iter in range(iteration):\n",
    "        '''loop through every node also in ontology (else just use estimation)'''\n",
    "        for word in loop_vocab:\n",
    "            '''get list of neighbor words (from Lexicon) that match the top N most similar word'''\n",
    "            word_neighbours = set(lexicon[word]).intersection(wvec_dict)\n",
    "            num_neighbours= len(word_neighbours)\n",
    "\n",
    "            if num_neighbours == 0:\n",
    "                continue\n",
    "\n",
    "            '''create new vec and estimate new vector according to neighbors'''\n",
    "            new_vec = num_neighbours * wvecs[word]\n",
    "            '''iterate every neighbor word and calculate according to equation'''\n",
    "            # hyperparameter\n",
    "            ALPHA = 0.8\n",
    "            for pp_word in word_neighbours:\n",
    "                dis = calculate_cosine_sim(new_wvecs[pp_word], wvecs[word])\n",
    "                new_vec += ((dis+ALPHA)*new_wvecs[pp_word])\n",
    "            new_wvecs[word] = new_vec/(2*num_neighbours)\n",
    "    return new_wvecs\n",
    "\n",
    "\n",
    "def retrofit_old(wvecs,lexicon,word,list_context,num_iters=10):\n",
    "\n",
    "    '''initialize new word vector'''\n",
    "    new_wvecs = wvecs\n",
    "\n",
    "    '''get top N words from GloVe that are most similar to word from text '''\n",
    "    wvec_dict = set(map(lambda k: k[0], wvecs.most_similar(word, topn=500)))\n",
    "\n",
    "    '''get list of mutual/intersected word between Lexicon and the N most similar words'''\n",
    "    loop_dict = wvec_dict.intersection(set(lexicon.keys()))\n",
    "\n",
    "    '''dict to store words as key and new vectors as value'''\n",
    "    result_vector={}\n",
    "\n",
    "    ''' iterate based on number of time we want to update'''\n",
    "    for iter in range(num_iters):\n",
    "        '''loop through every node also in ontology (else just use data estimate)'''\n",
    "        for word_sub in loop_dict:\n",
    "            '''get list of neighbor words (from Lexicon) that match the top N most similar word'''\n",
    "            word_neighbours = set(lexicon[word_sub]).intersection(wvec_dict)\n",
    "            num_neighbours = len(word_neighbours)\n",
    "\n",
    "            '''if words in list of mutual word do not have neighbor word, we just use estimate (no retrofit)'''\n",
    "            if num_neighbours == 0:\n",
    "                continue\n",
    "            '''create new vec and estimate new vector according to neighbors'''\n",
    "            new_vec = num_neighbours * wvecs.query(word_sub)\n",
    "            '''iterate every neighbor word and calculate according to equation'''\n",
    "            for pp_word in word_neighbours: # lexical synonym\n",
    "                new_vec += new_wvecs.query(pp_word)\n",
    "            result_vector[word_sub]=new_vec/(2*num_neighbours)\n",
    "\n",
    "    '''get word vector of interested word in text'''\n",
    "    vector_mainWord = wvecs.query(word)\n",
    "    '''create new dict that stores calculated cosine similarity between new word vector with interested word vector '''\n",
    "    dict_similarity_result= {}\n",
    "    for word,vector in result_vector.items():\n",
    "        dict_similarity_result[word] = calculate_cosine_sim(vector_mainWord, vector)\n",
    "\n",
    "    '''sort result dict by similarity'''\n",
    "    dict_similarity_result={k: v for k, v in sorted(dict_similarity_result.items(), key=lambda item: item[1],reverse=True)}\n",
    "\n",
    "    '''return to list of the most similar word'''\n",
    "    list_most_similar_word = list(dict_similarity_result.keys())\n",
    "    return list_most_similar_word\n",
    "\n",
    "'''Helper function'''\n",
    "def calculate_cosine_sim(vect1,vect2):\n",
    "    return dot(vect1, vect2)/(norm(vect1)*norm(vect2))\n",
    "\n",
    "''' Read the Lexicon (word relations) as a dictionary '''\n",
    "is_number = re.compile(r'\\d+.*')\n",
    "def norm_word(word):\n",
    "    if is_number.search(word.lower()):\n",
    "        return '---num---'\n",
    "    elif re.sub(r'\\W+', '', word) == '':\n",
    "        return '---punc---'\n",
    "    else:\n",
    "        return word.lower()\n",
    "\n",
    "def read_lexicon(filename):\n",
    "    '''Read and format Lexicon files'''\n",
    "    lexicon = {}\n",
    "    for line in open(filename, 'r',encoding='utf-8-sig'):\n",
    "        words = line.lower().strip().split()\n",
    "        lexicon[norm_word(words[0])] = [norm_word(word) for word in words[1:]]\n",
    "    return lexicon\n",
    "\n",
    "def save_embedding(retrofitted_vec, saved_file):\n",
    "    ''' Save embedded vectors to file in Glove format '''\n",
    "    output_file= open(saved_file, 'w',encoding='utf-8')\n",
    "\n",
    "    '''write 1 word per line with value and space in between '''\n",
    "    for word, values in retrofitted_vec.items():\n",
    "        output_file.write(word+' ')\n",
    "\n",
    "        for val in retrofitted_vec[word]:\n",
    "            output_file.write('%.4f' %(val)+' ')\n",
    "\n",
    "        output_file.write('\\n')\n",
    "\n",
    "    output_file.close()\n",
    "\n",
    "def load_Glove_to_dict(file_glove):\n",
    "    '''Load Glove to dictionary (key=word,value=vector)'''\n",
    "    print(\"Start loading Glove Model from Stanford Glove.txt\")\n",
    "\n",
    "    file = open(file_glove,'r',encoding='utf-8')\n",
    "    glove_dict = {}\n",
    "\n",
    "    for line in tqdm.tqdm(file):\n",
    "        split_lines = line.split()\n",
    "        word = split_lines[0]\n",
    "        word_embedding = np.array([float(value) for value in split_lines[1:]])\n",
    "        glove_dict[word] = word_embedding\n",
    "\n",
    "    print(len(glove_dict),\" words of Glove loaded successful!\")\n",
    "    return glove_dict\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    optparser = optparse.OptionParser()\n",
    "    optparser.add_option(\"-i\", \"--inputfile\", dest=\"input\", default=os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data', 'input',\n",
    "                                              'dev.txt'), help=\"input file with target word in context\")\n",
    "    # optparser.add_option(\"-w\", \"--wordvecfile\", dest=\"wordvecfile\", default=os.path.join('data', 'glove.6B.100d.magnitude'), help=\"word vectors file\")\n",
    "    optparser.add_option(\"-w\", \"--wordvecfile\", dest=\"wordvecfile\", default=os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub','answer', 'data', 'glove.6B.100d.txt'), help=\"word vectors file\")\n",
    "    optparser.add_option(\"-n\", \"--topn\", dest=\"topn\", default=10, help=\"produce these many guesses\")\n",
    "    optparser.add_option(\"-l\", \"--logfile\", dest=\"logfile\", default=None, help=\"log file for debugging\")\n",
    "    optparser.add_option(\"-L\", \"--lexiconfile\", dest=\"lexicon\", default=os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data',\n",
    "                                              'lexicons', 'wordnet-synonyms.txt'), help=\"lexicon file\")\n",
    "    optparser.add_option(\"-r\", \"--retrofitted_vecfile\", dest=\"retrofitted_vecfile\", default=os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data',\n",
    "                                              'retrofitted.glove.magnitude'), help=\"load retrofited embedding\")\n",
    "\n",
    "    (opts, _) = optparser.parse_args()\n",
    "\n",
    "    if opts.logfile is not None:\n",
    "        logging.basicConfig(filename=opts.logfile, filemode='w', level=logging.DEBUG)\n",
    "\n",
    "    '''get lexicon and format it'''\n",
    "    lexicon = read_lexicon(opts.lexicon)\n",
    "\n",
    "    ''' if we don't have retrofitted file, we retrofit else we just load.\n",
    "        We retrofitted Glove then save its embedding for later use (txt file) '''\n",
    "    if os.path.isfile(opts.retrofitted_vecfile) == False:\n",
    "\n",
    "        print(\"\\nRetrofitted embedding file does not exist. Let's retrofit !\\n\")\n",
    "\n",
    "        glove_dict = load_Glove_to_dict(opts.wordvecfile)\n",
    "        retrofitted_vec = retrofit(glove_dict,lexicon, iteration=10)\n",
    "\n",
    "        file_loc = 'data/retrofitted_glove.txt'\n",
    "        save_embedding(retrofitted_vec,file_loc)\n",
    "\n",
    "        print('\\nSuccessfully retrofitting embedding! and save to {}.'.format(file_loc))\n",
    "        print('-'*50)\n",
    "        print(\"PLEASE run pymagnitude.converter to convert .txt file to .magnitude file\")\n",
    "        print('-'*50)\n",
    "\n",
    "    else:\n",
    "        lexsub = LexSub(opts.retrofitted_vecfile, int(opts.topn))\n",
    "\n",
    "        num_lines = sum(1 for line in open(opts.input,'r',encoding='utf-8-sig'))\n",
    "\n",
    "        with open(opts.input,encoding='utf-8-sig') as f:\n",
    "            # for line in tqdm.tqdm(f, total=num_lines):\n",
    "            for line in f:\n",
    "                fields = line.strip().split('\\t')\n",
    "                print(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run our solution on the dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "edge front bottom line corner left back way middle then\n",
      "edge front bottom line corner left back way middle then\n",
      "edge front bottom line corner left back way middle then\n",
      "edge front bottom line corner left back way middle then\n",
      "edge front bottom line corner left back way middle then\n",
      "edge front bottom line corner left back way middle then\n",
      "edge front bottom line corner left back way middle then\n",
      "edge front bottom line corner left back way middle then\n",
      "edge front bottom line corner left back way middle then\n",
      "edge front bottom line corner left back way middle then\n"
     ]
    }
   ],
   "source": [
    "from lexsub import *\n",
    "import os\n",
    "\n",
    "\n",
    "lexsub = LexSub(os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data',\n",
    "                                              'retrofitted.glove.magnitude'))\n",
    "output = []\n",
    "with open(os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data', 'input',\n",
    "                                              'dev.txt')) as f:\n",
    "    for line in f:\n",
    "        fields = line.strip().split('\\t')\n",
    "        output.append(\" \".join(lexsub.substitutes(int(fields[0].strip()), fields[1].strip().split())))\n",
    "print(\"\\n\".join(output[:10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Score=42.40\n"
     ]
    }
   ],
   "source": [
    "from lexsub_check import precision\n",
    "with open(os.path.join('/Users', 'wusiyu', 'Desktop', 'nlp-class-hw', 'lexsub', 'data', 'reference','dev.out'), 'rt') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "print(\"Score={:.2f}\".format(100*precision(ref_data, output)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing we try is retrofitting. \n",
    "After applying retrofitting, the accuracy of our code could reach almost 40. And after that we try to modify the weight of our model to improve the performance. We select the cosine distance as one criteria in our model, and we also find an arbitrary weight. After changing the weight, the accuracy could exceed 42.\n",
    "We tried to consider context words, and use some models to generate new vectors. However, it turn out to have a negative impact on our result. And different approaches of selecting context words, could have totally different results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
