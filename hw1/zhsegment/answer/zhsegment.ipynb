{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# zhsegment: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Path to file (please specify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Siyu Path\n",
    "PATH = \"/Users/wusiyu/Desktop/nlp-class-hw/zhsegment/\"\n",
    "\n",
    "# Nattapat Path\n",
    "PATH = \"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白 国 良 、 夏儒阁 ）\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。\n",
      "上午 在 这里 签字 的 是 知识 信息 网络 通讯 技术 和 脱 氧 核 糖 核 酸 生物 技术 两 个 项目 ， 同时 还 签订 了 语言 教 学 交流 合作 协议 。\n",
      "这 三 个 项目 是 分别 由 国务院 发展 研究 中心 国际 技术 经济 研究所 上海 分 所 和 上海市 浦东 继续 教育 中心 ， 与 美国 知识 信息 网络 公司 、 世界 学习 组织 、 海 赛 克 公司 签订 的 。\n"
     ]
    }
   ],
   "source": [
    "Pw = Pdist(data=datafile(PATH+'data/count_1w.txt'))\n",
    "\n",
    "segmenter = Segment(Pw) # note that the default solution for this homework ignores the unigram counts\n",
    "output_full = []\n",
    "\n",
    "with open(PATH+\"data/input/dev.txt\",encoding='utf8') as f:\n",
    "    \n",
    "    for line in f:\n",
    "        output = \" \".join(segmenter.segment(line.strip()))\n",
    "        output_full.append(output)\n",
    "print(\"\\n\".join(output_full[:5])) # print out the first three lines of output as a sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Default score: 0.27\n"
     ]
    }
   ],
   "source": [
    "from zhsegment_check import fscore\n",
    "with open(PATH+'data/reference/dev.out', 'r',encoding='utf8') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "    tally = fscore(ref_data, output_full)\n",
    "    print(\"Default score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here.\n",
    "We are going to show our program below. \n",
    "For comparing with the default program, we will output the first three lines of the dev.txt as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中 美 在 沪 签订 高 科技 合作 协议 \n",
      "\n",
      "新华社 上海 八月 三十一日 电 （ 记者 白 国 良 、 夏儒阁 ） \n",
      "\n",
      "“ 中 美 合作 高 科技 项目 签字 仪式 ” 今天 在 上海 举行 。 \n",
      "\n",
      "上午 在 这里 签字 的 是 知识 信息 网络 通讯 技术 和 脱 氧 核 糖 核 酸 生物 技术 两 个 项目 ， 同时 还 签订 了 语言 教 学 交流 合作 协议 。 \n",
      "\n",
      "这 三 个 项目 是 分别 由 国务院 发展 研究 中心 国际 技术 经济 研究所 上海 分 所 和 上海市 浦东 继续 教育 中心 ， 与 美国 知识 信息 网络 公司 、 世界 学习 组织 、 海 赛 克 公司 签订 的 。 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "import re, string, random, glob, operator, heapq, codecs, sys, optparse, os, logging, math\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "from math import log10\n",
    "\n",
    "# additional library\n",
    "import operator\n",
    "\n",
    "INDEX_WORD = 0\n",
    "INDEX_STARTPOS = 1\n",
    "INDEX_PROBABILITY = 2\n",
    "INDEX_BACKPOINTER = 3\n",
    "\n",
    "class Segment:\n",
    "\n",
    "    def __init__(self, Pw):\n",
    "        self.Pw = Pw\n",
    "\n",
    "    def segment(self, text):\n",
    "        \"Return a list of words that is the best segmentation of text.\"\n",
    "        if not text: return []\n",
    "       \n",
    "        # call iterative_segmentation function\n",
    "        segmentation = iterative_segmentation(text,self.Pw,self.Pwords)\n",
    "\n",
    "        return segmentation\n",
    "\n",
    "    def Pwords(self, words):\n",
    "        \"The Naive Bayes probability of a sequence of words.\"\n",
    "        return self.Pw(words)\n",
    "        #return product(self.Pw(w) for w in words)\n",
    "\n",
    "def product(nums):\n",
    "    \"Return the product of a sequence of numbers.\"\n",
    "    return reduce(operator.mul, nums, 1)\n",
    "\n",
    "#### Support functions (p. 224)\n",
    "def iterative_segmentation(text,Pw,Pwords):\n",
    "    '''Iterative segmentation function, return list of segmented text'''\n",
    "\n",
    "    def heappush_list(h, item, key=lambda x: x):\n",
    "        '''push entry to heap'''\n",
    "        heapq.heappush(h, (key(item), item))\n",
    "        \n",
    "    def heappop_list(h):\n",
    "        ''' pop out entry from heap'''\n",
    "        return heapq.heappop(h)[1]\n",
    "    \n",
    "    def check_prev_entry(current_entry,chart):\n",
    "        ''' check whether there is previous entry existing in chart already or not'''\n",
    "        if current_entry[INDEX_STARTPOS] in chart:\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def get_prev_entry(current_entry,chart):\n",
    "        ''' return previous entry if it exists '''\n",
    "        if current_entry[INDEX_STARTPOS] in chart:\n",
    "            return chart[current_entry[INDEX_STARTPOS]]\n",
    "        return 'Error'\n",
    "    \n",
    "    def exist_in_heap(heap,entry):\n",
    "        ''' check whether there is previous entry existing in heap already or not'''\n",
    "        for entry_h in heap:\n",
    "            if entry_h[1][INDEX_WORD] == entry[INDEX_WORD]:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    '''Initialize the HEAP'''\n",
    "    heap = []\n",
    "    for pword,value in dict(Pw).items():\n",
    "\n",
    "        MAX_WORD_LENGTH = 8\n",
    "        for word_length in range(1,MAX_WORD_LENGTH):\n",
    "            # get the first word\n",
    "            if (text[:word_length] == pword[0:word_length]) and len(pword)==word_length:\n",
    "                # multiply by -1 to cast into positive\n",
    "                # then we can get Min Heap (minimum value at the top of heap)\n",
    "                each_entry = [pword,word_length-1,-1.0*log10(Pwords(pword)),None]\n",
    "                # push entry into the heap, sorted based on probability\n",
    "                heappush_list(heap, each_entry, key=operator.itemgetter(INDEX_PROBABILITY)) # sort by prob\n",
    "\n",
    "    '''if HEAP is still empty, we add smoothing '''\n",
    "    if len(heap) == 0 :\n",
    "        # smoothing 1/size of dictionary\n",
    "        smoothing_pro = 1 / len(list(dict(Pw).items()))\n",
    "        entry_add = [text[0], 0, smoothing_pro, None]\n",
    "        \n",
    "        heappush_list(heap, entry_add, key=operator.itemgetter(INDEX_PROBABILITY))\n",
    "\n",
    "    '''Iteratively fill in CHART for all i '''\n",
    "    chart = {}\n",
    "    count = 0\n",
    "    \n",
    "    while heap:\n",
    "\n",
    "        # get top entry from the heap\n",
    "        entry = heappop_list(heap)\n",
    "        # multiply -1 back to get original value of prob (original = negative log prob)\n",
    "        entry[INDEX_PROBABILITY] = -1.0*entry[INDEX_PROBABILITY]\n",
    "\n",
    "        # init endindex = entry starting position\n",
    "        endindex = entry[INDEX_STARTPOS]\n",
    "\n",
    "        '''iterate and decide whether to add words to heap '''\n",
    "        for pword,value in dict(Pw).items():\n",
    "\n",
    "            # break if there is no more text\n",
    "            if endindex+1 >= len(text):\n",
    "                break\n",
    "\n",
    "            # match word from dict based on the first index with new text\n",
    "            if pword[0] == text[endindex+1]:\n",
    "\n",
    "                # if (pword in text):\n",
    "                if (pword in text[endindex+1:endindex+1+len(pword)]):\n",
    "\n",
    "                    new_entry = [pword, endindex + len(pword), -1.0 * (entry[INDEX_PROBABILITY] + log10(Pwords(pword))),\n",
    "                                     entry[INDEX_STARTPOS]]\n",
    "\n",
    "                    # don't add new word if it is equal to popped word\n",
    "                    if pword == entry[INDEX_WORD]:\n",
    "                        continue\n",
    "                        \n",
    "                    # if word is already in chart, don't add to heap\n",
    "                    if check_prev_entry(new_entry,chart):\n",
    "                        continue\n",
    "                        \n",
    "                    # if word is in heap already, don't add\n",
    "                    if exist_in_heap(heap,new_entry):\n",
    "                        continue\n",
    "                    else:\n",
    "                        # add new word to heap\n",
    "                        heappush_list(heap, new_entry, key=operator.itemgetter(INDEX_PROBABILITY))  # sort by prob\n",
    "\n",
    "        ''' add smoothing for word that does not appear in dict'''\n",
    "        if len(heap) == 0 and endindex < len(text)-1:\n",
    "            smoothing_pro = 1 / len(list(dict(Pw).items()))\n",
    "            entry_add = [text[endindex+1], endindex+1, smoothing_pro, endindex]\n",
    "            \n",
    "            heappush_list(heap, entry_add, key=operator.itemgetter(INDEX_PROBABILITY))\n",
    "\n",
    "        if chart and check_prev_entry(entry,chart):\n",
    "            # get previous entry\n",
    "            previous_entry = get_prev_entry(entry,chart)\n",
    "\n",
    "            # if assign popped entry to chart belonging to previous entry\n",
    "            # if popped entry probability > previous entry probability\n",
    "            if entry[INDEX_PROBABILITY] > previous_entry[INDEX_PROBABILITY]:\n",
    "                chart[endindex] = entry\n",
    "\n",
    "            # if popped entry probability <= previous entry probability, do nothing\n",
    "            if entry[INDEX_PROBABILITY] <= previous_entry[INDEX_PROBABILITY]:\n",
    "                count += 1\n",
    "                continue\n",
    "        else:\n",
    "            # add popped word to chart\n",
    "            chart[endindex] = entry\n",
    "\n",
    "        count += 1\n",
    "\n",
    "    return get_segmented_text(chart)\n",
    "\n",
    "def get_segmented_text(dict_text):\n",
    "    ''' Get list of word from Dynamic programming table (chart) '''\n",
    "    # if dict_text is empty, we return empty list\n",
    "    if len(dict_text) < 1:\n",
    "        return []\n",
    "\n",
    "    last_entry = dict_text[max(list(dict_text.keys()))]\n",
    "    list_result = []\n",
    "\n",
    "    # get last element\n",
    "    list_result.append(last_entry[INDEX_WORD])\n",
    "    # get pointer from last element\n",
    "    ptr_idx = last_entry[INDEX_BACKPOINTER]\n",
    "\n",
    "    # loop while backpoint is not None\n",
    "    while ptr_idx != None:\n",
    "        entry = dict_text[ptr_idx]\n",
    "        list_result.append(entry[INDEX_WORD])\n",
    "        ptr_idx = entry[INDEX_BACKPOINTER]\n",
    "\n",
    "    #reverse list\n",
    "    list_result = list_result[::-1]\n",
    "    return list_result\n",
    "\n",
    "\n",
    "class Pdist(dict):\n",
    "    \"A probability distribution estimated from counts in datafile.\"\n",
    "    def __init__(self, data=[], N=None, missingfn=None):\n",
    "        for key,count in data:\n",
    "            self[key] = self.get(key, 0) + int(count)\n",
    "        self.N = float(N or sum(self.values()))\n",
    "        self.missingfn = missingfn or (lambda k, N: 1./N)\n",
    "    def __call__(self, key):\n",
    "        if key in self: return self[key]/self.N\n",
    "        else: return self.missingfn(key, self.N)\n",
    "\n",
    "def datafile(name, sep='\\t'):\n",
    "    \"Read key,value pairs from file.\"\n",
    "    with open(name,encoding=\"utf8\") as fh:\n",
    "    # with open(name) as fh:\n",
    "        for line in fh:\n",
    "            (key, value) = line.split(sep)\n",
    "            yield (key, value)\n",
    "\n",
    "def punish_long_words(key, Pw):\n",
    "    return (1. / Pw.N) ** len(key)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    Pw = Pdist(data=datafile(PATH+\"data/count_1w.txt\"),missingfn=punish_long_words)\n",
    "    \n",
    "    segmenter = Segment(Pw)\n",
    "    \n",
    "    i = 1\n",
    "    with open(PATH+\"data/input/dev.txt\",encoding='utf8') as f:\n",
    "        for line in f:\n",
    "            sentence =\" \".join(segmenter.segment(line.strip()))\n",
    "            print(sentence,'\\n')\n",
    "            if i ==5:\n",
    "                break\n",
    "            i += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative segmentation: Dev score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Iterative segmentation score: 0.86\n"
     ]
    }
   ],
   "source": [
    "# ref data\n",
    "with open(PATH+'data/reference/dev.out', 'r',encoding='utf8') as refh:\n",
    "    ref_data = [str(x).strip() for x in refh.read().splitlines()]\n",
    "\n",
    "# iterative segemented output\n",
    "with open(PATH+\"dev_output/dev.out\",'r',encoding='utf8') as devh:\n",
    "    iterative_dev_data = [str(x).strip() for x in devh.read().splitlines()]\n",
    "        \n",
    "tally = fscore(ref_data, iterative_dev_data)\n",
    "print(\"Iterative segmentation score: {:.2f}\".format(tally), file=sys.stderr)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?\n",
    "\n",
    "We have done many steps to increase the accuracy of Chinese word segmentation. First of all, we wrote up the Iterative Segmentation function to replace the default function. By using Iterative Segmenter, the segmenter will not only identify each Chinese character as a word, but also start to combine some single Chinese character into one word. Secondly, we have also done the Smoothing work. After all, not every word exists in our dictionary, which means that some word could have a probability of 0. To avoid this situation, we include add-one smoothing method in our code. After smmothing, our code works even better as shown in new dev score."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
