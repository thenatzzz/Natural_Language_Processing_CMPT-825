{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group: Oasis\n",
    "### Members: \n",
    "### 1. Nattapat Juthaprachakul, njuthapr\n",
    "### 2. Siyu Wu, sw246"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chunker: default program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from default import *\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the default solution on dev"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [00:02<00:00, 459.66it/s]\n"
     ]
    }
   ],
   "source": [
    "chunker = LSTMTagger(os.path.join('data', 'train.txt.gz'), os.path.join('data', 'chunker'), '.tar')\n",
    "decoder_output = chunker.decode('data/input/dev.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the default output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 11672 phrases; correct: 8568.\n",
      "accuracy:  84.35%; (non-O)\n",
      "accuracy:  85.65%; precision:  73.41%; recall:  72.02%; FB1:  72.71\n",
      "             ADJP: precision:  36.49%; recall:  11.95%; FB1:  18.00  74\n",
      "             ADVP: precision:  71.36%; recall:  39.45%; FB1:  50.81  220\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  70.33%; recall:  76.80%; FB1:  73.42  6811\n",
      "               PP: precision:  92.40%; recall:  87.14%; FB1:  89.69  2302\n",
      "              PRT: precision:  65.00%; recall:  57.78%; FB1:  61.18  40\n",
      "             SBAR: precision:  84.62%; recall:  41.77%; FB1:  55.93  117\n",
      "               VP: precision:  63.66%; recall:  58.25%; FB1:  60.83  2108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(73.40644276901988, 72.02420981842637, 72.70875763747455)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('data','reference','dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Documentation\n",
    "\n",
    "Write some beautiful documentation of your program here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "INDEX_BEG = 0\n",
    "LENGTH_VECTOR_ENCODING = 300\n",
    "# dtype = torch.cuda.FloatTensor\n",
    "torch.set_default_tensor_type('torch.cuda.FloatTensor')\n",
    "\n",
    "\n",
    "################################################################\n",
    "'''Helper functions'''\n",
    "\n",
    "def encode_one_char(sentence,vector_2d,first_char=True):\n",
    "    ''' vector size = string.printable == 100\n",
    "     one-hot encoding only the first/last letter of word '''\n",
    "\n",
    "    '''if word exists in string.printable, use 1; otherwise, 0'''\n",
    "    # iterate word by word in the sentence\n",
    "    for word,vector in zip(sentence,vector_2d):\n",
    "        if word == '[UNK]':\n",
    "            continue\n",
    "\n",
    "        if first_char:\n",
    "            index_word = INDEX_BEG\n",
    "        else:\n",
    "            index_word = len(word)-1\n",
    "\n",
    "        letter = word[index_word]\n",
    "        index_strPrintable= string.printable.find(letter) # letter to index\n",
    "        vector[index_strPrintable] = 1.0\n",
    "    return vector_2d\n",
    "\n",
    "def encode_internal_chars(sentence,vector_2d):\n",
    "    ''' vector size = string.printable == 100\n",
    "     Encoding only the internal letters of word  (excluding begining and ending chars)'''\n",
    "\n",
    "    # iterate word by word in the sentence\n",
    "    for word,vector in zip(sentence,vector_2d):\n",
    "        if word == '[UNK]':\n",
    "            continue\n",
    "\n",
    "        # if word length < 3, there is no internal\n",
    "        if len(word) < 3:\n",
    "            continue\n",
    "\n",
    "        internal_word = word[1:-1]\n",
    "        # interate letter in the internal word\n",
    "        for letter in internal_word:\n",
    "            index_strPrintable= string.printable.find(letter) # letter to index\n",
    "            vector[index_strPrintable] += 1.0\n",
    "    return vector_2d\n",
    "\n",
    "def encoding_sentence(sentence):\n",
    "    '''Function to encode every word in the sentence'''\n",
    "\n",
    "    '''encoding the beginning charactor of all words in the sentence'''\n",
    "    beginChar_vector = np.zeros((len(sentence),len(string.printable)))\n",
    "    beginChar_vector = encode_one_char(sentence,beginChar_vector,first_char=True)\n",
    "\n",
    "    '''encoding the ending charactor of all words in the sentence'''\n",
    "    endChar_vector = np.zeros((len(sentence),len(string.printable)))\n",
    "    endChar_vector = encode_one_char(sentence,endChar_vector,first_char=False)\n",
    "\n",
    "    '''encoding all internal charactors of all words in the sentence'''\n",
    "    internal_vector = np.zeros((len(sentence),len(string.printable)))\n",
    "    internal_vector = encode_internal_chars(sentence,internal_vector)\n",
    "\n",
    "    ''' concate all 3 vectors '''\n",
    "    encoding_vector = np.concatenate((beginChar_vector,internal_vector,endChar_vector),axis=1)\n",
    "    # print(beginChar_vector.shape,endChar_vector.shape,internal_vector.shape,encoding_vector.shape)\n",
    "\n",
    "    '''create Tensor from numpy object '''\n",
    "    # encoding_tensor = torch.tensor(encoding_vector, dtype=torch.float)\n",
    "    encoding_tensor = torch.tensor(encoding_vector, dtype=torch.float).cuda()\n",
    "\n",
    "    return encoding_tensor\n",
    "############################################################\n",
    "\n",
    "\n",
    "class LSTMTaggerModel(nn.Module):\n",
    "\n",
    "    def __init__(self, embedding_dim, hidden_dim, vocab_size, tagset_size,char_encoding):\n",
    "        torch.manual_seed(1)\n",
    "        super(LSTMTaggerModel, self).__init__()\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.word_embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "\n",
    "        # The LSTM takes word embeddings as inputs, and outputs hidden states\n",
    "        # with dimensionality hidden_dim.\n",
    "        '''if using character-level encoding, hidden dim to lstm = 128+300 = 428'''\n",
    "        if char_encoding:\n",
    "            lstm_embedding_dim = embedding_dim+LENGTH_VECTOR_ENCODING\n",
    "        else:\n",
    "            lstm_embedding_dim = embedding_dim\n",
    "\n",
    "        self.lstm = nn.LSTM(lstm_embedding_dim, hidden_dim, bidirectional=False)\n",
    "\n",
    "        ## second LSTM for character-level encoding vector\n",
    "        # self.lstm_encoding = nn.LSTM(LENGTH_VECTOR_ENCODING, hidden_dim, bidirectional=False)\n",
    "\n",
    "        # The linear layer that maps from hidden state space to tag space\n",
    "        self.hidden2tag = nn.Linear(hidden_dim, tagset_size)\n",
    "\n",
    "    def forward(self, sentence,encoding_tensor=None):\n",
    "        embeds = self.word_embeddings(sentence)\n",
    "\n",
    "        # # put character-level encoding vectors into LSTM before concatenating with embedding\n",
    "        # if encoding_tensor is not None:\n",
    "        #     reshaped_encoding_tensor = torch.reshape(encoding_tensor,(-1,1,LENGTH_VECTOR_ENCODING))\n",
    "        #     lstm_out_encoding, _ = self.lstm_encoding(reshaped_encoding_tensor)\n",
    "        #     encoding_tensor = torch.reshape(lstm_out_encoding,(-1,LENGTH_VECTOR_ENCODING))\n",
    "\n",
    "        '''if using character-level encoding, we concatenate Embedding vector with new encoded vectors = 128+300 = 428'''\n",
    "        if encoding_tensor is not None:\n",
    "            embeds = torch.cat([embeds,encoding_tensor],dim=1)\n",
    "\n",
    "        lstm_out, _ = self.lstm(embeds.view(len(sentence), 1, -1))\n",
    "        tag_space = self.hidden2tag(lstm_out.view(len(sentence), -1))\n",
    "        tag_scores = F.log_softmax(tag_space, dim=1)\n",
    "        return tag_scores\n",
    "\n",
    "class LSTMTagger:\n",
    "\n",
    "    def __init__(self, trainfile, modelfile, modelsuffix, unk=\"[UNK]\", epochs=10, embedding_dim=128, hidden_dim=64,char_encoding=False):\n",
    "        self.unk = unk\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.epochs = epochs\n",
    "        self.modelfile = modelfile\n",
    "        self.modelsuffix = modelsuffix\n",
    "        self.training_data = []\n",
    "        if trainfile[-3:] == '.gz':\n",
    "            with gzip.open(trainfile, 'rt') as f:\n",
    "                self.training_data = read_conll(f)\n",
    "        else:\n",
    "            with open(trainfile, 'r') as f:\n",
    "                self.training_data = read_conll(f)\n",
    "\n",
    "        self.word_to_ix = {} # replaces words with an index (one-hot vector)\n",
    "        self.tag_to_ix = {} # replace output labels / tags with an index\n",
    "        self.ix_to_tag = [] # during inference we produce tag indices so we have to map it back to a tag\n",
    "\n",
    "        for sent, tags in self.training_data:\n",
    "            for word in sent:\n",
    "                if word not in self.word_to_ix:\n",
    "                    self.word_to_ix[word] = len(self.word_to_ix)\n",
    "            for tag in tags:\n",
    "                if tag not in self.tag_to_ix:\n",
    "                    self.tag_to_ix[tag] = len(self.tag_to_ix)\n",
    "                    self.ix_to_tag.append(tag)\n",
    "\n",
    "        logging.info(\"word_to_ix:\", self.word_to_ix)\n",
    "        logging.info(\"tag_to_ix:\", self.tag_to_ix)\n",
    "        logging.info(\"ix_to_tag:\", self.ix_to_tag)\n",
    "\n",
    "        '''Flag whether do character-level encoding or not'''\n",
    "        self.char_encoding = char_encoding\n",
    "        # self.model = LSTMTaggerModel(self.embedding_dim, self.hidden_dim, len(self.word_to_ix), len(self.tag_to_ix))\n",
    "        self.model = LSTMTaggerModel(self.embedding_dim, self.hidden_dim, len(self.word_to_ix), len(self.tag_to_ix),char_encoding=char_encoding).cuda()\n",
    "\n",
    "        self.optimizer = optim.SGD(self.model.parameters(), lr=0.01)\n",
    "\n",
    "\n",
    "    def train(self):\n",
    "        loss_function = nn.NLLLoss()\n",
    "\n",
    "        self.model.train()\n",
    "        loss = float(\"inf\")\n",
    "        for epoch in range(self.epochs):\n",
    "            for sentence, tags in tqdm.tqdm(self.training_data):\n",
    "\n",
    "                # Step 1. Remember that Pytorch accumulates gradients.\n",
    "                # We need to clear them out before each instance\n",
    "                self.model.zero_grad()\n",
    "\n",
    "                # Step 2. Get our inputs ready for the network, that is, turn them into\n",
    "                # Tensors of word indices.\n",
    "                # sentence_in = prepare_sequence(sentence, self.word_to_ix, self.unk)\n",
    "                sentence_in = prepare_sequence(sentence, self.word_to_ix, self.unk).cuda()\n",
    "\n",
    "                # targets = prepare_sequence(tags, self.tag_to_ix, self.unk)\n",
    "                targets = prepare_sequence(tags, self.tag_to_ix, self.unk).cuda()\n",
    "\n",
    "                # Step 3. Run our forward pass.\n",
    "                # tag_scores = self.model(sentence_in)\n",
    "                '''if using character-level encoding, we encode the tensor'''\n",
    "                # create character level vectors to concate with the embeddings\n",
    "                if self.char_encoding:\n",
    "                    encoding_tensor = encoding_sentence(sentence)\n",
    "                else:\n",
    "                    encoding_tensor = None\n",
    "\n",
    "                tag_scores = self.model(sentence_in,encoding_tensor=encoding_tensor)\n",
    "\n",
    "                # Step 4. Compute the loss, gradients, and update the parameters by\n",
    "                #  calling optimizer.step()\n",
    "                loss = loss_function(tag_scores, targets)\n",
    "                loss.backward()\n",
    "                self.optimizer.step()\n",
    "\n",
    "            if epoch == self.epochs-1:\n",
    "                epoch_str = '' # last epoch so do not use epoch number in model filename\n",
    "            else:\n",
    "                epoch_str = str(epoch)\n",
    "            savefile = self.modelfile + epoch_str + self.modelsuffix\n",
    "            print(\"saving model file: {}\".format(savefile), file=sys.stderr)\n",
    "            torch.save({\n",
    "                        'epoch': epoch,\n",
    "                        'model_state_dict': self.model.state_dict(),\n",
    "                        'optimizer_state_dict': self.optimizer.state_dict(),\n",
    "                        'loss': loss,\n",
    "                        'unk': self.unk,\n",
    "                        'word_to_ix': self.word_to_ix,\n",
    "                        'tag_to_ix': self.tag_to_ix,\n",
    "                        'ix_to_tag': self.ix_to_tag,\n",
    "                    }, savefile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code explaination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important codes that we have implemented are the encoding_sentence where we encode each word in the sentence onto 3 vector where the first and second vectors represent the first and last character (encode_one_char) while the last vector represents the internal characters (encode_internal_chars)(all character between first and last character). After we have these 3 encoded vectors, we joined them with the word embedding. Also, we have tried putting these encoded vectors into LSTM of 64 or 100 hidden units before joing with word embedding (however, we comment out this code since this technique does not improve our score)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run our solution on the dev.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1027/1027 [00:01<00:00, 617.66it/s]\n"
     ]
    }
   ],
   "source": [
    "from chunker import *\n",
    "import os\n",
    "\n",
    "optparser = optparse.OptionParser()\n",
    "optparser.add_option(\"-i\", \"--inputfile\", dest=\"inputfile\", default=os.path.join('/Users/','wusiyu','Desktop','nlp-class-hw','chunker','data', 'input', 'dev.txt'), help=\"produce chunking output for this input file\")\n",
    "optparser.add_option(\"-t\", \"--trainfile\", dest=\"trainfile\", default=os.path.join('/Users/','wusiyu','Desktop','nlp-class-hw','chunker','data', 'train.txt.gz'), help=\"training data for chunker\")\n",
    "optparser.add_option(\"-m\", \"--modelfile\", dest=\"modelfile\", default=os.path.join('/Users/','wusiyu','Desktop','nlp-class-hw','chunker','data', 'chunker'), help=\"filename without suffix for model files\")\n",
    "optparser.add_option(\"-s\", \"--modelsuffix\", dest=\"modelsuffix\", default='.tar', help=\"filename suffix for model files\")\n",
    "optparser.add_option(\"-e\", \"--epochs\", dest=\"epochs\", default=5, help=\"number of epochs [fix at 5]\")\n",
    "optparser.add_option(\"-u\", \"--unknowntoken\", dest=\"unk\", default='[UNK]', help=\"unknown word token\")\n",
    "optparser.add_option(\"-f\", \"--force\", dest=\"force\", action=\"store_true\", default=False, help=\"force training phase (warning: can be slow)\")\n",
    "optparser.add_option(\"-l\", \"--logfile\", dest=\"logfile\", default=None, help=\"log file for debugging\")\n",
    "optparser.add_option(\"-o\", \"--outputfile\", dest=\"outputfile\", default='output.txt', help=\"print result to output file\")\n",
    "\n",
    "(opts, _) = optparser.parse_args()\n",
    "modelfile = opts.modelfile\n",
    "\n",
    "\n",
    "chunker = LSTMTagger(opts.trainfile, modelfile, opts.modelsuffix, opts.unk,char_encoding=True)\n",
    "decoder_output = chunker.decode(opts.inputfile)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate our output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "processed 23663 tokens with 11896 phrases; found: 11968 phrases; correct: 9279.\n",
      "accuracy:  87.23%; (non-O)\n",
      "accuracy:  88.26%; precision:  77.53%; recall:  78.00%; FB1:  77.77\n",
      "             ADJP: precision:  43.33%; recall:  17.26%; FB1:  24.68  90\n",
      "             ADVP: precision:  70.99%; recall:  46.73%; FB1:  56.36  262\n",
      "            CONJP: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "             INTJ: precision:   0.00%; recall:   0.00%; FB1:   0.00  0\n",
      "               NP: precision:  76.23%; recall:  81.80%; FB1:  78.92  6693\n",
      "               PP: precision:  91.11%; recall:  87.34%; FB1:  89.19  2340\n",
      "              PRT: precision:  66.67%; recall:  48.89%; FB1:  56.41  33\n",
      "             SBAR: precision:  84.38%; recall:  45.57%; FB1:  59.18  128\n",
      "               VP: precision:  69.78%; recall:  73.35%; FB1:  71.52  2422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(77.5317513368984, 78.00100874243444, 77.76567214213878)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "flat_output = [ output for sent in decoder_output for output in sent ]\n",
    "import conlleval\n",
    "true_seqs = []\n",
    "with open(os.path.join('/Users/','wusiyu','Desktop','nlp-class-hw','chunker','data','reference','dev.out')) as r:\n",
    "    for sent in conlleval.read_file(r):\n",
    "        true_seqs += sent.split()\n",
    "conlleval.evaluate(true_seqs, flat_output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis\n",
    "\n",
    "Do some analysis of the results. What ideas did you try? What worked and what did not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most important thing we implemented is that we concatenated the word embedding input to the chunker RNN as an input vector that is the character-level represenation of the word. To be more specific, we used three types of vectors (each with the same size of 100) to represent a word, which are the vector of the first character, the vector of the last character, and the vector of internal characters. Then, we just concatenated these three vectors to generate the final character-level represenation of a word. \n",
    "\n",
    "For the result of our method, there are lots of improvement to the scores. The FB1 score increases from 72.7 by the default method to 77.7 by our method. Also, we have tried putting the character-level vectors into LSTM with hidden layer of 64 and 100; however, the results are worse than the original one that we implemented.\n",
    "\n",
    "For the training procedure, we have trained these models with Nvidia GPU 1050Ti and these are the results and training time: \n",
    "\n",
    "    Default->  FB1:72.7, training time: 18mins\n",
    "    Encoded character-level vectors-> FB1:77.766, training time: 18 mins\n",
    "    Encoded character-level vectors with LSTM (64 hidden units)-> FB1:76.79, training time:28 mins\n",
    "    Encoded character-level vectors with LSTM (100 hidden units)-> FB1: 76.62, training time:30 mins\n",
    " \n",
    "Therefore, this results shows that Encoded character-level vectors with word embedding are the best method in both training time and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
